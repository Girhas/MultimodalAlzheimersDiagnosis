{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 16:59:15.508205: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-16 16:59:15.516832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 16:59:15.526037: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 16:59:15.528808: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-16 16:59:15.536377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 16:59:15.938201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold  # Import KFold for cross-validation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import ConvNeXtBase\n",
    "from tensorflow.keras.layers import Dropout, GlobalAveragePooling2D, Dense, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ConvNeXtBase\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Define data loading and preprocessing\n",
    "PATH = 'AD Data'\n",
    "max_samples_per_class = 5000  # Maximum samples per class\n",
    "\n",
    "def load_samples_from_directory(directory_path, max_samples_per_class):\n",
    "    class_data = {}  # Dictionary to hold samples for each class\n",
    "\n",
    "    for class_name in os.listdir(directory_path):\n",
    "        class_path = os.path.join(directory_path, class_name)\n",
    "        \n",
    "        if os.path.isdir(class_path):  # Check if it is a directory\n",
    "            files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            if len(files) > max_samples_per_class:\n",
    "                files = np.random.choice(files, max_samples_per_class, replace=False)  # Randomly select samples\n",
    "            class_data[class_name] = files[:max_samples_per_class]  # Limit to max_samples_per_class\n",
    "    \n",
    "    return class_data\n",
    "\n",
    "# Load samples from the directory\n",
    "class_data = load_samples_from_directory(PATH, max_samples_per_class)\n",
    "\n",
    "# Prepare data for K-Fold\n",
    "all_images = []  # List to store images\n",
    "all_labels = []  # List to store labels\n",
    "\n",
    "for class_name, files in class_data.items():\n",
    "    for filename in files:\n",
    "        image_path = os.path.join(PATH, class_name, filename)\n",
    "        img = tf.keras.utils.load_img(image_path, target_size=(160, 160))  # Adjust target_size based on your model input\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        all_images.append(img_array)\n",
    "        all_labels.append(class_name)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_images = np.array(all_images)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Convert labels to numerical values\n",
    "class_names = np.unique(all_labels)\n",
    "label_to_index = {class_name: index for index, class_name in enumerate(class_names)}\n",
    "all_labels = np.array([label_to_index[label] for label in all_labels])\n",
    "\n",
    "# Define K-Fold cross-validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(hp):\n",
    "    base_model = ConvNeXtBase(weights='imagenet', include_top=False, input_shape=(160, 160, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    units = hp.Int('units', min_value=64, max_value=256, step=64)\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    \n",
    "    skip = Dense(units, activation='relu')(x)\n",
    "    x = Add()([x, skip])\n",
    "    \n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    predictions = Dense(len(class_names), activation='softmax')(x)\n",
    "    model = keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 02m 00s]\n",
      "val_accuracy: 0.8389999866485596\n",
      "\n",
      "Best val_accuracy So Far: 0.8389999866485596\n",
      "Total elapsed time: 00h 02m 00s\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kathy/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.7676 - loss: 0.4906 - val_accuracy: 0.8350 - val_loss: 0.4051\n",
      "Epoch 2/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.7780 - loss: 0.4596 - val_accuracy: 0.8415 - val_loss: 0.3975\n",
      "Epoch 3/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.7994 - loss: 0.4269 - val_accuracy: 0.8490 - val_loss: 0.3717\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8396 - loss: 0.3508\n",
      "Fold 1 - Validation Accuracy: 0.8489999771118164\n",
      "New best model found in fold 1 with accuracy 0.8489999771118164. Saving the model...\n",
      "Training fold 2/5...\n",
      "Epoch 1/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.7809 - loss: 0.4646 - val_accuracy: 0.8425 - val_loss: 0.3797\n",
      "Epoch 2/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.7919 - loss: 0.4404 - val_accuracy: 0.8420 - val_loss: 0.3643\n",
      "Epoch 3/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8032 - loss: 0.4230 - val_accuracy: 0.8370 - val_loss: 0.3779\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.7747 - loss: 0.4572\n",
      "Fold 2 - Validation Accuracy: 0.8370000123977661\n",
      "Training fold 3/5...\n",
      "Epoch 1/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.7804 - loss: 0.4649 - val_accuracy: 0.8375 - val_loss: 0.3849\n",
      "Epoch 2/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.7908 - loss: 0.4402 - val_accuracy: 0.8470 - val_loss: 0.3701\n",
      "Epoch 3/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8086 - loss: 0.4189 - val_accuracy: 0.8585 - val_loss: 0.3572\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8293 - loss: 0.3557\n",
      "Fold 3 - Validation Accuracy: 0.8585000038146973\n",
      "New best model found in fold 3 with accuracy 0.8585000038146973. Saving the model...\n",
      "Training fold 4/5...\n",
      "Epoch 1/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.7832 - loss: 0.4578 - val_accuracy: 0.8425 - val_loss: 0.3858\n",
      "Epoch 2/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.7930 - loss: 0.4423 - val_accuracy: 0.8455 - val_loss: 0.3806\n",
      "Epoch 3/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8015 - loss: 0.4382 - val_accuracy: 0.8540 - val_loss: 0.3673\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8123 - loss: 0.3899\n",
      "Fold 4 - Validation Accuracy: 0.8539999723434448\n",
      "Training fold 5/5...\n",
      "Epoch 1/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.7975 - loss: 0.4448 - val_accuracy: 0.8265 - val_loss: 0.4061\n",
      "Epoch 2/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8027 - loss: 0.4432 - val_accuracy: 0.8390 - val_loss: 0.3892\n",
      "Epoch 3/3\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8031 - loss: 0.4316 - val_accuracy: 0.8485 - val_loss: 0.3656\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8028 - loss: 0.3966\n",
      "Fold 5 - Validation Accuracy: 0.8485000133514404\n",
      "Mean Validation Accuracy across 5 folds: 0.849399995803833\n",
      "Standard Deviation of Validation Accuracy: 0.007193045277239458\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# K-Fold Cross-Validation Loop\n",
    "validation_scores = []\n",
    "best_overall_accuracy = 0\n",
    "best_overall_model = None\n",
    "best_model_path = 'best_overall_model_ConvNeXtSmall.keras'  # Path to save the best model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=1,\n",
    "    executions_per_trial=1,\n",
    "    directory='ConvNeXtSmall_tuning',\n",
    "    project_name='ConvNeXtSmall_tuning'\n",
    ")\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(all_images)):\n",
    "    print(f'Training fold {fold + 1}/{num_folds}...')\n",
    "    \n",
    "    X_train, X_val = all_images[train_index], all_images[val_index]\n",
    "    y_train, y_val = all_labels[train_index], all_labels[val_index]\n",
    "    \n",
    "    # Create TensorFlow datasets for the current fold\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32).shuffle(buffer_size=1000)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32)\n",
    "\n",
    "    # Find the best model using the tuner\n",
    "    tuner.search(train_ds, epochs=10, validation_data=val_ds)\n",
    "    best_models = tuner.get_best_models(num_models=1)\n",
    "\n",
    "    if len(best_models) > 0:\n",
    "        best_model = best_models[0]\n",
    "\n",
    "        # Fit the best model on the current training fold\n",
    "        history = best_model.fit(train_ds, epochs=3, validation_data=val_ds, verbose=1)\n",
    "        \n",
    "        # Evaluate on the validation fold\n",
    "        val_loss, val_accuracy = best_model.evaluate(val_ds)\n",
    "        validation_scores.append(val_accuracy)\n",
    "        print(f'Fold {fold + 1} - Validation Accuracy: {val_accuracy}')\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if val_accuracy > best_overall_accuracy:\n",
    "            best_overall_accuracy = val_accuracy\n",
    "            best_overall_model = best_model\n",
    "            # Save the best model\n",
    "            print(f'New best model found in fold {fold + 1} with accuracy {val_accuracy}. Saving the model...')\n",
    "            best_model.save(best_model_path)\n",
    "\n",
    "    else:\n",
    "        print(\"No models were found by the tuner for this fold.\")\n",
    "    \n",
    "    # Clear the Keras session after each fold\n",
    "    K.clear_session()\n",
    "\n",
    "# Summary of validation scores\n",
    "print(f'Mean Validation Accuracy across {num_folds} folds: {np.mean(validation_scores)}')\n",
    "print(f'Standard Deviation of Validation Accuracy: {np.std(validation_scores)}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
